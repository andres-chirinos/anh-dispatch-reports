{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ff8254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (4.13.4)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (from beautifulsoup4) (4.14.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/andreschirinos/Proyectos/anh-dispatch-reports/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install --upgrade pip \n",
    "%pip install pandas beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a779eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import os, json, re, hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3639ba22",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "input_files = [\"raw_html_despachosCBBA.html\",\"raw_html_despachoSCZ.html\",\"raw_html_reportedespachos.html\"]\n",
    "data_dir = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "490bb16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table found!\n",
      "Table headers: ['destino', 'localidad', 'producto', 'hora_salida', 'hora_llegada']\n",
      "Number of rows extracted: 34\n",
      "Extracted date and time: 15/06/2025 15:01:56\n",
      "Table found!\n",
      "Table headers: ['destino', 'localidad', 'producto', 'hora_salida', 'hora_llegada']\n",
      "Number of rows extracted: 66\n",
      "Extracted date and time: 15/06/2025 15:01:10\n",
      "Table found!\n",
      "Table headers: ['destino', 'localidad', 'producto', 'hora_salida', 'hora_llegada_aprox']\n",
      "Number of rows extracted: 25\n",
      "Extracted date and time: 15/06/2025 15:00:27\n",
      "Combined data saved to ../data/dispatches.csv\n"
     ]
    }
   ],
   "source": [
    "for input_file in input_files:\n",
    "    with open(os.path.join(data_dir,input_file), 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    soup = bs(html_content, \"html.parser\")\n",
    "    # Find the table in the HTML\n",
    "    table = soup.find(\"table\")\n",
    "    if table:\n",
    "        print(\"Table found!\")\n",
    "    else:\n",
    "        print(\"No table found in the HTML content.\")\n",
    "\n",
    "    # Extract the table headers\n",
    "    headers = [header.text.strip().lower().replace(\" \", \"_\").replace(\".\",\"\") for header in table.find_all(\"th\")]\n",
    "    print(\"Table headers:\", headers)\n",
    "\n",
    "    # Extract the table rows\n",
    "    rows = []\n",
    "\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        cells = row.find_all(\"td\")\n",
    "        if cells:\n",
    "            rows.append([cell.text.strip() for cell in cells])\n",
    "    print(\"Number of rows extracted:\", len(rows))\n",
    "\n",
    "    # Display the first few rows of the DataFrame\n",
    "    obtain_time_selector = \"html body div.container div.footer p\"\n",
    "    obtain_time = soup.select_one(obtain_time_selector)\n",
    "    pattern = r\"\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2}\"\n",
    "    match = re.search(pattern, obtain_time.text)\n",
    "    if match:\n",
    "        print(\"Extracted date and time:\", match.group())\n",
    "\n",
    "    # Acumular las filas de todos los archivos y añadir la fuente\n",
    "    if 'all_data' not in globals():\n",
    "        all_data = []\n",
    "    all_headers = headers + ['source', 'timestamp']  # Asumimos que los headers son iguales en todos los archivos y añadimos columnas para la fuente y timestamp\n",
    "    rows_with_source = [row + [input_file, match.group()] for row in rows]\n",
    "    all_data.extend(rows_with_source)\n",
    "\n",
    "    # Si es el último archivo, crear el DataFrame combinado y guardarlo a CSV\n",
    "    if input_file == input_files[-1]:\n",
    "        import pandas as pd\n",
    "        df = pd.DataFrame(all_data, columns=all_headers)\n",
    "        output_file = os.path.join(data_dir, \"dispatches.csv\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d6a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain metadata\n",
    "def obtain_metadata(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        file_content = f.read()\n",
    "    file_hash = hashlib.sha256(file_content).hexdigest()\n",
    "    return {\n",
    "        \"file_name\": os.path.basename(file_path),\n",
    "        \"file_size\": os.path.getsize(file_path),\n",
    "        \"file_hash\": file_hash\n",
    "    }\n",
    "metadata = obtain_metadata(os.path.join(data_dir, \"dispatches.csv\"))\n",
    "\n",
    "# Save metadata to a JSON file\n",
    "metadata_file = os.path.join(data_dir, \"dispatches_metadata.json\")\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "print(f\"Metadata saved to {metadata_file}\")\n",
    "\n",
    "# Display metadata\n",
    "print(\"Metadata:\")\n",
    "print(json.dumps(metadata, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
